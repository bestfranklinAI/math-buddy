#!/usr/bin/env python3
"""
Interactive configuration script for Math Buddy
This script helps users set up their .env file based on their preferred setup.
"""

import os
import shutil

def get_yes_no(prompt: str, default: bool = False) -> bool:
    """Get yes/no input from user"""
    default_str = "Y/n" if default else "y/N"
    while True:
        response = input(f"{prompt} ({default_str}): ").strip().lower()
        if not response:
            return default
        if response in ['y', 'yes']:
            return True
        elif response in ['n', 'no']:
            return False
        print("Please enter 'y' or 'n'")

def get_input(prompt: str, default: str = "") -> str:
    """Get text input from user"""
    if default:
        response = input(f"{prompt} (default: {default}): ").strip()
        return response if response else default
    else:
        return input(f"{prompt}: ").strip()

def print_header(text: str):
    """Print a formatted header"""
    print(f"\n{'='*50}")
    print(f"  {text}")
    print(f"{'='*50}")

def main():
    print("üßÆ Math Buddy Configuration Setup")
    print("This script will help you create your .env file")
    
    # Check if .env already exists
    env_path = ".env"
    if os.path.exists(env_path):
        if not get_yes_no("‚ö†Ô∏è  .env file already exists. Overwrite?", False):
            print("Aborted. Existing .env file preserved.")
            return
    
    print_header("Model Preferences")
    
    # LLM preferences
    print("\nüß† Language Model (LLM) Setup:")
    print("Choose how you want to handle text generation:")
    print("  1. Local Ollama (free, private, requires setup)")
    print("  2. GitHub Models API (high quality, requires token)")
    print("  3. Mock responses (for testing)")
    
    while True:
        llm_choice = input("\nEnter your choice (1/2/3): ").strip()
        if llm_choice in ['1', '2', '3']:
            break
        print("Please enter 1, 2, or 3")
    
    use_local_llm = llm_choice == '1'
    use_cloud_llm = llm_choice == '2'
    
    # Image preferences
    print("\nüé® Image Generation Setup:")
    print("Choose how you want to handle image generation:")
    print("  1. Local DrawThings/SD WebUI (free, private, requires setup)")
    print("  2. HuggingFace API (high quality, requires token)")
    print("  3. Mock images (for testing)")
    
    while True:
        img_choice = input("\nEnter your choice (1/2/3): ").strip()
        if img_choice in ['1', '2', '3']:
            break
        print("Please enter 1, 2, or 3")
    
    use_local_image = img_choice == '1'
    use_cloud_image = img_choice == '2'
    
    # Collect configuration based on choices
    config = {}
    
    # LLM configuration
    config['USE_LOCAL_LLM'] = 'true' if use_local_llm else 'false'
    
    if use_local_llm:
        print_header("Local LLM Configuration")
        config['OLLAMA_URL'] = get_input("Ollama URL", "http://localhost:11434")
        config['OLLAMA_MODEL'] = get_input("Ollama Model", "phi4-mini-reasoning:latest")
    
    if use_cloud_llm:
        print_header("GitHub Models Configuration")
        print("Get your token at: https://github.com/settings/tokens")
        config['GITHUB_TOKEN'] = get_input("GitHub Personal Access Token")
    
    # Image configuration
    config['USE_LOCAL_IMAGE'] = 'true' if use_local_image else 'false'
    
    if use_local_image:
        print_header("Local Image Configuration")
        config['DRAWTHINGS_URL'] = get_input("DrawThings/SD WebUI URL", "http://127.0.0.1:7860/sdapi/v1/txt2img")
    
    if use_cloud_image:
        print_header("HuggingFace Configuration")
        print("Get your token at: https://huggingface.co/settings/tokens")
        config['HF_TOKEN'] = get_input("HuggingFace API Token")
        config['HF_API_URL'] = get_input("HF API URL (optional)", "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3-medium-diffusers")
    
    # General configuration
    print_header("General Configuration")
    config['BACKEND_URL'] = get_input("Backend URL", "http://localhost:8000")
    
    # Generate .env file
    print_header("Generating Configuration")
    
    env_content = []
    env_content.append("# Math Buddy Backend Configuration")
    env_content.append("# Generated by configure.py")
    env_content.append("")
    
    # LLM section
    env_content.append("# ======================")
    env_content.append("# LLM Configuration")
    env_content.append("# ======================")
    env_content.append(f"USE_LOCAL_LLM={config['USE_LOCAL_LLM']}")
    
    if 'OLLAMA_URL' in config:
        env_content.append(f"OLLAMA_URL={config['OLLAMA_URL']}")
        env_content.append(f"OLLAMA_MODEL={config['OLLAMA_MODEL']}")
    
    if 'GITHUB_TOKEN' in config:
        env_content.append(f"GITHUB_TOKEN={config['GITHUB_TOKEN']}")
    
    env_content.append("")
    
    # Image section
    env_content.append("# ======================")
    env_content.append("# Image Configuration")
    env_content.append("# ======================")
    env_content.append(f"USE_LOCAL_IMAGE={config['USE_LOCAL_IMAGE']}")
    
    if 'DRAWTHINGS_URL' in config:
        env_content.append(f"DRAWTHINGS_URL={config['DRAWTHINGS_URL']}")
    
    if 'HF_TOKEN' in config:
        env_content.append(f"HF_TOKEN={config['HF_TOKEN']}")
        if config.get('HF_API_URL'):
            env_content.append(f"HF_API_URL={config['HF_API_URL']}")
    
    env_content.append("")
    
    # General section
    env_content.append("# ======================")
    env_content.append("# General Configuration")
    env_content.append("# ======================")
    env_content.append(f"BACKEND_URL={config['BACKEND_URL']}")
    
    # Write .env file
    with open(env_path, 'w') as f:
        f.write('\n'.join(env_content) + '\n')
    
    print(f"‚úÖ Configuration saved to {env_path}")
    
    # Provide next steps
    print_header("Next Steps")
    
    if use_local_llm:
        print("üß† For Local LLM:")
        print("  1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh")
        print(f"  2. Pull model: ollama pull {config.get('OLLAMA_MODEL', 'phi4-mini-reasoning:latest')}")
        print("  3. Start Ollama: ollama serve")
    
    if use_local_image:
        print("\nüé® For Local Images:")
        print("  ‚Ä¢ macOS: Install DrawThings from App Store")
        print("  ‚Ä¢ Other: Set up Stable Diffusion WebUI with --api flag")
    
    if use_cloud_llm or use_cloud_image:
        print("\n‚òÅÔ∏è For Cloud APIs:")
        print("  ‚Ä¢ Ensure your API tokens are valid")
        print("  ‚Ä¢ Check token permissions and quotas")
    
    print("\nüîß Test Your Setup:")
    print("  python test_local_models.py")
    
    print("\nüöÄ Start the Backend:")
    print("  cd ..")
    print("  uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000")
    
    print("\nüìñ For more details, see:")
    print("  ‚Ä¢ README.md (this directory)")
    print("  ‚Ä¢ ../SETUP.md (project root)")

if __name__ == "__main__":
    main()
