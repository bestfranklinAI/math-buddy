# Backend Environment Variables

# ======================
# LLM Configuration
# ======================

# Use local Ollama instead of GitHub Models (optional - defaults to false)
USE_LOCAL_LLM=false

# Ollama configuration (only used if USE_LOCAL_LLM=true)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi4-mini-reasoning:latest

# GitHub Token for LLM API access (required if USE_LOCAL_LLM=false)
GITHUB_TOKEN=your_github_token_here

# ======================
# Image Generation Configuration
# ======================

# Use local DrawThings instead of HuggingFace (optional - defaults to false)
USE_LOCAL_IMAGE=false

# DrawThings configuration (only used if USE_LOCAL_IMAGE=true)
DRAWTHINGS_URL=http://127.0.0.1:7860/sdapi/v1/txt2img

# Hugging Face Token for image generation (required if USE_LOCAL_IMAGE=false)
HF_TOKEN=your_huggingface_token_here

# Hugging Face API URL (optional - defaults to stable diffusion)
HF_API_URL=https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3-medium-diffusers

# ======================
# General Configuration
# ======================

# Backend URL for image serving (defaults to localhost:8000)
BACKEND_URL=http://localhost:8000

# In production, you might use:
# BACKEND_URL=https://your-api-domain.com

# ======================
# Local Setup Examples
# ======================

# For fully local setup (no API keys needed):
# USE_LOCAL_LLM=true
# USE_LOCAL_IMAGE=true

# For hybrid setup (local LLM, cloud image generation):
# USE_LOCAL_LLM=true
# USE_LOCAL_IMAGE=false
# HF_TOKEN=your_huggingface_token_here

# For cloud setup (requires API keys):
# USE_LOCAL_LLM=false
# USE_LOCAL_IMAGE=false
# GITHUB_TOKEN=your_github_token_here
# HF_TOKEN=your_huggingface_token_here
